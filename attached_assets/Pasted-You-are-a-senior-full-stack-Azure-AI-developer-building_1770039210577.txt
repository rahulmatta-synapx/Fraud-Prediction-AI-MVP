You are a senior full-stack Azure AI developer building production-grade MVPs. Create a complete starter blueprint + code for a solo-developed MVP: Fraud Prediction AI Agent (UK Motor Insurance focus) using Option A – Rules-First + LLM Assist.

Key principles (must enforce in all code/design):
- This is ONLY a decision-support helper/co-pilot for fraud analysts in UK motor insurance claims.
- The AI NEVER makes or enforces decisions — the fraud score (0-100) is just a recommendation/risk indicator to help prioritize investigations.
- Investigators MUST be able to freely change/override the score if they disagree (no enforcement of AI suggestion).
- Overrides require: new score (0-100), mandatory reason (dropdown like "False positive", "Additional evidence", "Disagree with signal", + free text notes).
- ALL changes (initial score calc, override, notes, status updates) are logged in immutable audit trail with user_id, timestamp, old_value, new_value, reason — queryable per claim.
- Human-in-the-loop 100%: final decision is always human; AI only suggests + explains (e.g., "This shows inconsistencies — review recommended").

Core flow:
1. Authenticated user submits claim (JSON structured data for motor insurance: policy_id, claim_amount, accident_date/location/type, vehicle_details, claimant_history etc. + documents: PDFs/images/emails like accident reports, invoices, medical reports, garage estimates).
2. Docs uploaded to storage → processed (OCR/extract).
3. LLM analyzes extracted text → outputs neutral signals (e.g. "date mismatch between description and invoice", "unusual high repair amount", "inconsistent injury description") + evidence snippets + confidence (no judgmental terms like "fraudulent" or "suspicious claim").
4. Rules engine applies configurable weighted rules on structured data + LLM signals → computes recommendation score 0-100.
5. Save score + explanations + signals to DB.
6. Dashboard shows: prioritized list of claims (sorted by score descending, with risk banding high/medium/low), detail with score, rule triggers, LLM evidence highlights, document viewer.
7. Investigator can: view, override score + reason/notes → save → log change → re-display updated (status can change e.g. to "reviewing" or "decided").
8. Support score updates if new documents arrive (trigger re-processing).

Tech stack — best 2026 choices for Azure + Replit development:
- Frontend: React 18+ (Vite) + Tailwind CSS + shadcn/ui or MUI + Axios + React Router + TanStack Query
- Backend: Python Azure Functions v4 (HTTP trigger) integrated with FastAPI (modern, type-safe, auto-docs) — best for Python serverless APIs in 2026
- Auth: Simple JWT (PyJWT) for MVP speed in Replit; stub for Azure Entra ID (comment how to swap later for RBAC: fraud-analyst, claims-manager roles)
- Document extraction: Azure AI Document Intelligence (layout + prebuilt-invoice/read models — ideal for invoices, scanned PDFs, forms in insurance claims)
- LLM: Azure OpenAI (gpt-4o-mini — cost-effective, fast reasoning)
- Storage: Azure Blob Storage (raw docs) + Azure Cosmos DB NoSQL (claims + audit-logs containers)
- Secrets: Replit secrets → Azure Key Vault (instructions to migrate)
- Deployment: Develop in Replit → Git push to GitHub → Azure Functions deploy via Azure CLI in Replit shell or GitHub Actions

Detailed output required (step-by-step guide + full code skeletons):

1. Azure Portal setup instructions (exact CLI commands where possible):
   - Resource group (UK South for residency/GDPR)
   - Azure OpenAI (gpt-4o-mini deployment)
   - Azure AI Document Intelligence (endpoint/key)
   - Blob Storage account + container 'claims-docs'
   - Cosmos DB account + db 'fraud-agent' + containers 'claims' (partition /claimId) + 'audit-logs' (partition /claimId)
   - Functions App (Python 3.11/3.12, Linux, Consumption plan)
   - Key Vault (store keys)

2. Project structure (Replit folders):
   - /frontend (React app)
   - /backend (Azure Functions + FastAPI)
   - /shared (Pydantic models if needed)

3. Data schemas (Pydantic):
   - ClaimInput (motor-specific fields), ClaimDocumented, LLMSignal, RuleConfig, FraudScoreResult, OverrideInput (new_score, reason_category, notes), AuditEntry

4. Backend code (Python + FastAPI in Functions):
   - requirements.txt (azure-ai-documentintelligence, openai, azure-cosmos, azure-identity, fastapi, pydantic, pyjwt etc.)
   - __init__.py for Functions (HTTP triggers)
   - FastAPI app mounted in Functions
   - Ingestion endpoint (POST /claims) with basic validation rules (e.g. required fields, amount >0)
   - Processing function (Document Intelligence + OpenAI call)
   - Detailed LLM prompt template (neutral: detect inconsistencies/mismatches only, motor insurance examples like accident date vs medical report, repair costs vs invoice, PII mask instructions)
   - Rules engine function (JSON config file or DB, weighted sum, configurable by business)
   - Score save to Cosmos
   - GET /claims (sorted high-risk desc, risk bands), GET /claims/{id}, POST /claims/{id}/override (log to audit)
   - Re-score endpoint/trigger for new docs
   - Audit log on every write (immutable)

5. Frontend React structure:
   - App.tsx with Router + AuthProvider
   - Login (simple form or Azure AD stub)
   - Dashboard: table (score, id, date, status, risk band) + filters/sort by score
   - ClaimDetail: score badge (color-coded), rules list, LLM signals with highlighted evidence, PDF viewer (react-pdf), override form (slider/input for new score, reason select + textarea, submit)
   - Use JWT from localStorage

6. Running locally in Replit:
   - Install deps (npm/py)
   - Set secrets (OPENAI_ENDPOINT, etc.)
   - Run backend (uvicorn or Functions host)
   - Run frontend (npm run dev)

7. Deployment steps:
   - From Replit shell: az login → az functionapp deployment source config-zip etc.
   - Or GitHub + Azure Pipelines
   - CORS setup
   - UK South region everywhere

8. Good-to-have features:
   - Claim status enum (new, scored, reviewing, decided)
   - Re-score trigger on new doc
   - Basic RBAC stub (analyst vs manager)
   - Basic metrics collection (App Insights stub: high-risk %, overrides count for future success metrics)
   - Export claims CSV
   - Error retry for AI calls
   - Rate limiting on API

Output everything:
- Numbered steps
- Full code blocks (requirements, main files, prompts, components)
- Gotchas (costs, OCR scanned doc issues, prompt sensitivity)
- Estimated solo time per section

Generate the complete, ready-to-deploy MVP blueprint now — production-minded, secure, auditable, human-centric.